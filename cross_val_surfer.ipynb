{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_latex(latex):\n",
    "    patterns = [\n",
    "        (r\"\\$(.*?)\\$\", lambda match: match.group(1)),  # Extract content between dollar signs\n",
    "        (r\"\\\\[a-zA-Z]+\\{([^\\}]*)\\}\", lambda match: match.group(1)),  # Remove backslash and braces around commands\n",
    "        (r\"\\^\\{(.*?)\\}\", lambda match: f\"^{match.group(1)}\"),  # Handle superscript\n",
    "        (r\"_\\{(.*?)\\}\", lambda match: f\"_{match.group(1)}\"),  # Handle subscript\n",
    "        (r\"\\$(.*?)\\$\", lambda match: match.group(1)),  # Extract content between dollar signs\n",
    "        (r\"\\\\([a-zA-Z]+)\", lambda match: match.group(1)),  # Remove backslash from commands\n",
    "        (r\"\\{([^}]*)\\}\", lambda match: match.group(1)),  # Remove braces around arguments\n",
    "        (r\"\\^\\{(.*?)\\}\", lambda match: f\"^{match.group(1)}\"),  # Handle superscript\n",
    "        (r\"_\\{(.*?)\\}\", lambda match: f\"_{match.group(1)}\"),  # Handle subscript\n",
    "    ]\n",
    "    \n",
    "    for pattern, repl in patterns:\n",
    "        latex = re.sub(pattern, repl, latex)\n",
    "    \n",
    "    return latex\n",
    "\n",
    "def cross_val_parser(URL):\n",
    "    parsed = []\n",
    "    response = requests.get(url=URL,)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    question = soup.find_all('div',{'id':'question-header'})[0].find_all('h1')[0].get_text()\n",
    "    parsed.append(translate_latex(question))\n",
    "    question_body = soup.find_all('div',{'class':'s-prose js-post-body'})[0].find_all('p')\n",
    "\n",
    "    question_body_ = ''\n",
    "    for paragraph in question_body:\n",
    "        question_body_ += paragraph.get_text()\n",
    "    parsed.append(translate_latex(question_body_))\n",
    "\n",
    "    answers = []\n",
    "    try:\n",
    "        answers_ = soup.find_all('div',{'id':'answers'})[0]\n",
    "        main_answer = soup.find_all('div',{'id':'answers'})[0].find_all('div',{'class':'answer js-answer accepted-answer js-accepted-answer'})[0].find_all('div',{'class':'s-prose js-post-body'})[0].find_all('p')[0].get_text()\n",
    "        answers.append(translate_latex(main_answer))\n",
    "\n",
    "        other_answers = soup.find_all('div',{'id':'answers'})[0].find_all('div',{'class':'answer js-answer'})\n",
    "        for element in other_answers:\n",
    "            body = element.find_all('div',{'class':'s-prose js-post-body'})[0]\n",
    "            answer = ''\n",
    "            for sub in body:\n",
    "                answer += sub.get_text()\n",
    "            answers.append(translate_latex(answer))\n",
    "        \n",
    "        parsed.append(answers)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_crawler(URL):\n",
    "    urls = []\n",
    "    domain = 'https://stats.stackexchange.com'\n",
    "    response = requests.get(url=URL,)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    try:\n",
    "        links = soup.find_all('div',{'class':'module sidebar-related'})[0].find_all('div',{'class':'related js-gps-related-questions'})[0].find_all('div',{'class':'spacer'})\n",
    "        for link in links:\n",
    "            url = link.find_all('a')\n",
    "            urls.append(domain+str(url[1]['href']))\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return urls\n",
    "    \n",
    "def cross_val_surfer(init, trigger):\n",
    "    output = [cross_val_parser(init)]\n",
    "    urls = set(cross_val_crawler(init))\n",
    "    while len(output) < trigger and len(urls) > 0:\n",
    "        url = list(urls).pop(0)\n",
    "        output.append(cross_val_parser(url))\n",
    "        urls.union(set(cross_val_crawler(url)))\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
