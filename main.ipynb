{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tools Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import euler_number\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1) Raccolta e etichettatura delle immagini\n",
    "\n",
    "Creiamo il nostro dataset di train e test etichettando le varie immagini e considerando varie situazioni: diverse posizioni degli oggetti, cambi di luminosità, presenza di ombre e diversi sfondi, alcuni dei quali presentano delle irregolarità."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10 diverse categorie:\n",
    "    - accendino\n",
    "    - cacciavite\n",
    "    - chiave\n",
    "    - forbici\n",
    "    - martello\n",
    "    - metro\n",
    "    - nastro\n",
    "    - pappagallo\n",
    "    - penna\n",
    "    - spillatrice \n",
    "\n",
    "- 4 diverse superfici:\n",
    "    - tavolo bianco\n",
    "    - banco da lavoro\n",
    "    - pavimento di cemento\n",
    "    - coperta a righe\n",
    "- 20 immagini per allenare il modello di classificazione per ogni categoria\n",
    "- 12 immagini per il test contenenti oggetti multipli\n",
    "\n",
    "Le gli oggetti non toccano i bordi e non si toccano fra di loro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oggetti utilizzati.\n",
    "tool_names = ['accendino', 'cacciavite', 'chiave', 'forbici', 'martello', 'metro', 'nastro', 'pappagallo', 'penna', 'spillatrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rinomiamo le immagini nelle cartelle.\n",
    "cwd = os.getcwd()\n",
    "for tool in tool_names:\n",
    "    path = os.path.join(cwd, f\"images/{tool}/\")\n",
    "    image_files = glob.glob(path + '*.png')\n",
    "\n",
    "    for i, file in enumerate(image_files):\n",
    "        new_name = tool+ '_' + str(i+1) + '.png'\n",
    "        os.rename(file, os.path.join(path, new_name))\n",
    "\n",
    "\n",
    "# Rinominiamo le immagini del test.\n",
    "path = os.path.join(cwd, f\"images/Test_set/\")\n",
    "image_files = glob.glob(path + '*.png')\n",
    "\n",
    "for i, file in enumerate(image_files):\n",
    "    new_name = 'test_' + str(i+1) + '.png'\n",
    "    os.rename(file, os.path.join(path, new_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) Pre-elaborazione delle immagini\n",
    "\n",
    "Prima della segmentazione creiamo un *ground truth* per ogni immagine in modo tale da poter valutare lo step successivo in temini di accuratezza e modifichiamo le immagini per preparale per i prossimi step: ridimensioniamo le immagini riducendole tutte alla stessa dimensione (520 x 520 pixel) e le ricoloriamo in scala di grigi per ridurle ad un singolo canale e renderle più pratiche e semplici da elaborare."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridimensionamento : portartiamo nello stesso formato tutte le immagini (520 x 520). Alcune delle utilità di questo passaggio possono essere una maggiore uniformità una volta che si va ad allenare l'algoritmo di calssificazione e l'aumento della diversità dei dati di training che possono portare il modello ad avere una maggiore robustezza;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portiamo tutte le immagini nella stessa dimensione (520 x 520).\n",
    "cwd = os.getcwd()\n",
    "\n",
    "for tool in tool_names:\n",
    "    path = os.path.join(cwd, f\"images/{tool}/\")\n",
    "    image_files = glob.glob(path + '*.png')\n",
    "\n",
    "    for i, file in enumerate(image_files):\n",
    "        image = Image.open(file)\n",
    "\n",
    "        # Ridimensionamento dell'immagine.\n",
    "        resized_image = image.resize((520, 520))\n",
    "        resized_image.save(file)\n",
    "\n",
    "# test-set\n",
    "path = os.path.join(cwd, f\"images/Test_set/\")\n",
    "image_files = glob.glob(path + '*.png')\n",
    "\n",
    "for i, file in enumerate(image_files):\n",
    "    image = Image.open(file)\n",
    "\n",
    "    # Ridimensionamento dell'immagine.\n",
    "    resized_image = image.resize((520, 520))\n",
    "    resized_image.save(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ricolorazione: applichiamo la scalda di grigi riducendo le immagini ad un unico canale rendendole più semplici da analizzare;\n",
    "- Gaussian Filter: applichiamo un `Gaussian filter` con dimensione 5x5 per sfocare l'immagine e rendere lo sfondo piu semplice da distinguere rispetto agli oggetti.\n",
    "\n",
    "Queste due tecninche le applichiamo direttamente prima della segmentazione nello ste successivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3) Segmentazione delle immagini \n",
    "\n",
    "Una volta ottenuto l'immagine preparata, si applica un metodo di segmentazione per dividere l'immagine in regioni omogenee. Abbiamo provato ad utilizzare tecniche di segmentazione come la segmentazione basata sulla soglia, la segmentazione basata sulla regione o la segmentazione basata sui cluster: queste tecniche non hanno portato buoni risultati perchè, anche se identificano molto bene gli oggetti con una precisione quasi sempre superiore al 90%, con altrettanta precisione sbagliano ad identificare lo sfondo a causa della presenza di ombre e di sfondi che non permetono una chiara distinzione con l'oggetto considerato e presentano delle imperfezioni. Confrontiamo perciò alcune tecniche di segmentazione sulle immagini a sfondo bianco."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tecniche di segmentazione:\n",
    "- per contorni: Canny edge detector\n",
    "\n",
    "- per regioni: sogliatura automatica OTSU\n",
    "\n",
    "- mediante clustering: k-mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tools = ['accendino', 'cacciavite', 'chiave', 'forbici', 'martello', 'metro', 'nastro', 'pappagallo', 'penna', 'spillatrice']\n",
    "tp_tot, tn_tot, fp_tot, fn_tot = [], [], [], []\n",
    "\n",
    "cwd = os.getcwd()\n",
    "folder_path = os.path.join(cwd, f\"images\")\n",
    "\n",
    "\n",
    "for tool in tqdm(tools):\n",
    "\n",
    "    with open(f\"{folder_path}\\{tool}\\{tool}_gt.json\", 'r') as f:\n",
    "        ground_truth_file = json.load(f)\n",
    "\n",
    "    for pos, el in enumerate(ground_truth_file):\n",
    "\n",
    "        if pos < 5:\n",
    "            truth_mask = np.zeros((520, 520))\n",
    "            image = cv2.imread(f\"{folder_path}\\{tool}\\{tool}_{pos+1}.png\")\n",
    "\n",
    "            for i in range(len(ground_truth_file[el]['regions'])):\n",
    "\n",
    "                x_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_x']\n",
    "                y_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_y']\n",
    "                vertices = np.array([[x_pos[u], y_pos[u]] for u in range(len(x_pos))])\n",
    "                if i == 0:\n",
    "                        cv2.fillPoly(truth_mask, [vertices], 255)\n",
    "                else:\n",
    "                        cv2.fillPoly(truth_mask, [vertices], 0)\n",
    "\n",
    "            # Ground truth.\n",
    "            x_pos = ground_truth_file[el]['regions'][0]['shape_attributes']['all_points_x']\n",
    "            y_pos = ground_truth_file[el]['regions'][0]['shape_attributes']['all_points_y']\n",
    "            vertices = np.array([[x_pos[i], y_pos[i]] for i in range(len(x_pos))])\n",
    "\n",
    "            # Preprocessing.\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (3, 3), cv2.BORDER_DEFAULT)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "            gray = cv2.erode(gray, kernel, iterations=3)\n",
    "\n",
    "\n",
    "            ## Tecniche di segmentazione.\n",
    "\n",
    "            # 1) Sogliaura di OTSU\n",
    "            _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "            # 2) K-means\n",
    "            small = cv2.pyrDown(gray)\n",
    "            Z = small.reshape((-1, 1))\n",
    "            Z = np.float32(Z)\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "            K = 2\n",
    "            ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "            center = np.uint8(center)\n",
    "            res = center[label.flatten()]\n",
    "            res2 = res.reshape((small.shape))\n",
    "            res2 = cv2.pyrUp(res2)\n",
    "            _, mask = cv2.threshold(res2, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "            # 3) CANNY Edge Detection\n",
    "            edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "            closed_edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "            mask = nd.binary_fill_holes(closed_edges)\n",
    "            mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "\n",
    "            # cv2.imshow(\"Masked Image\", mask)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "            # Ground truth mask.\n",
    "            truth_mask = truth_mask.astype(bool).flatten()\n",
    "            opening = mask.astype(bool)\n",
    "\n",
    "            # Confronto tra segmentazione e ground truth.\n",
    "            ground_truth = truth_mask.flatten()\n",
    "            predictions = [elem for elem in opening.flatten()]\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(ground_truth, predictions, labels=[True, False]).ravel()\n",
    "            tp_tot.append(tp)\n",
    "            tn_tot.append(tn)\n",
    "            fp_tot.append(fp)\n",
    "            fn_tot.append(fn)\n",
    "\n",
    "\n",
    "# Matrice di Confusione.\n",
    "sum_tp, sum_tn, sum_fp, sum_fn = sum(tp_tot), sum(tn_tot), sum(fp_tot), sum(fn_tot) \n",
    "\n",
    "cf_matrix = np.array([[sum_tp/(sum_tp+sum_fp), sum_fp/(sum_tp+sum_fp)],\n",
    "                       [sum_fn/(sum_tn+sum_fn), sum_tn/(sum_tn+sum_fn)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Tool', 'Background']\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='viridis',  xticklabels=labels, yticklabels=labels, cbar=False, fmt=\".1%\")\n",
    "plt.title(\"Title\")\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Prove_segmentazione\\Canny_edge_detection.png\"  width=\"30%\" height=\"30%\">\n",
    "<img src=\"Prove_segmentazione\\k_means.png\"  width=\"30%\" height=\"30%\">\n",
    "<img src=\"Prove_segmentazione\\Sogliatura_Otsu.png\"  width=\"30%\" height=\"30%\">\n",
    "\n",
    "\n",
    "Problemi:\n",
    "- **Ombre**, andando a segmentare l'immagine le ombre tendono ad essere classificate come l'oggetto e infatti notiamo come per tutti i metodi ci sia una buona percentuale di falsi positivi, ovvero lo sfondo viene riconosciuto erroneamente come oggetto;\n",
    "- **Sfondo**, gli sfondi utilizzati creano diversi problemi alle tecinche di segmentazione che non riescono adeliminare i pattern presenti, es. i pallini del tavolo da lavoro o le strisce della coperta, e portano perciò ad una segmentazione degli oggetti erronea, aumetanto notevolmente la percentuale di pixel classificati come oggetti quando questi appartengono allo sfondo.\n",
    "\n",
    "\n",
    "''' Confrontiamo alcune tecninche di segmentazione sulle immagini a sfondo bianco (vedi Prove Tecninche di Segmentazione) e notiamo come la sogliatura per contorni risulti la migliore perchè riesce ad andare ad identificare in aniera abbastanza corretta i bordi delle immagini non classificando lo sfondo come oggetto. Continuiamo la nostra analisi utilizzando il ground truth per l'estazione delle features e la classificazinoe degli oggetti.'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4) Estrazione delle features\n",
    "\n",
    "Le features che andiamo ad estrarre da ogni immagine per allenare in nostro modello sono:\n",
    "\n",
    "- `Numero di Eulero`: verifica la presenza di buchi all'interno dell'immagine segmentata;\n",
    "- `Momenti di Hu`: set di sette momenti invarianti alle trasformazioni di scala, rotazione e traslazione;\n",
    "- `Rapporto degli assi dell'ellisse`: rapporto degli assi dell'ellisse all'interno del quale è contenuta l'immagine segmentata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "tools = ['accendino', 'cacciavite', 'chiave', 'forbici', 'martello', 'metro','nastro', 'pappagallo', 'penna', 'spillatrice']\n",
    "feat_euler, feat_hu, feat_axis = [], [], []\n",
    "y_labels = []\n",
    "\n",
    "# Caricamento dell'immagine segmentata\n",
    "cwd = os.getcwd()\n",
    "folder_path = os.path.join(cwd, f\"images\")\n",
    "\n",
    "for tool in tqdm(tools):\n",
    "\n",
    "    with open(f\"{folder_path}\\{tool}\\{tool}_gt.json\", 'r') as f:\n",
    "        ground_truth_file = json.load(f)\n",
    "\n",
    "    for pos, el in enumerate(ground_truth_file):\n",
    "\n",
    "        truth_mask = np.zeros((520, 520))\n",
    "        image = cv2.imread(f\"{folder_path}\\{tool}\\{tool}_{pos+1}.png\")\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        for i in range(len(ground_truth_file[el]['regions'])):\n",
    "\n",
    "            x_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_x']\n",
    "            y_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_y']\n",
    "            vertices = np.array([[x_pos[u], y_pos[u]] for u in range(len(x_pos))])\n",
    "            if i == 0:\n",
    "                    cv2.fillPoly(truth_mask, [vertices], 255)\n",
    "            else:\n",
    "                    cv2.fillPoly(truth_mask, [vertices], 0)\n",
    "\n",
    "        # Ground truth.\n",
    "        x_pos = ground_truth_file[el]['regions'][0]['shape_attributes']['all_points_x']\n",
    "        y_pos = ground_truth_file[el]['regions'][0]['shape_attributes']['all_points_y']\n",
    "        vertices = np.array([[x_pos[i], y_pos[i]] for i in range(len(x_pos))])\n",
    "        truth_mask = truth_mask.astype(bool)\n",
    "\n",
    "        # Immagine segmentata.\n",
    "        segmented = np.zeros_like(image)\n",
    "        segmented[truth_mask] = image[truth_mask]\n",
    "        \n",
    "\n",
    "        ### Estrazione delle Features.\n",
    "        binary_mask = cv2.cvtColor(segmented, cv2.COLOR_BGR2GRAY)\n",
    "        _, binary_mask = cv2.threshold(binary_mask, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Numero di Eulero.\n",
    "        euler = euler_number(binary_mask)\n",
    "\n",
    "        # Momenti di Hu.\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        moments = cv2.moments(binary_mask)\n",
    "        hu_moments  = cv2.HuMoments(moments).flatten()\n",
    "\n",
    "        # Rapporto assi ellisse.\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        ellipse = cv2.fitEllipse(largest_contour)\n",
    "        major_axis, minor_axis = ellipse[1]\n",
    "        axis_ratio = major_axis / minor_axis \n",
    "\n",
    "\n",
    "        y_labels.append(tool)\n",
    "        feat_euler.append(euler)\n",
    "        feat_hu.append(hu_moments )\n",
    "        feat_axis.append(axis_ratio)\n",
    "\n",
    "all_features = np.column_stack((feat_euler, feat_hu, feat_axis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a normalizzare il il valore delle feature con il numero di pixel di una dimensione dell'immagine originale in modo da poter classificare anche immagini di dimensioni diverse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5) Addestramento del modello di classificazione \n",
    "\n",
    "Come modello di classificazione multicalsse utilizziamo il `Decision Tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dati.\n",
    "X = all_features\n",
    "y = y_labels\n",
    "\n",
    "# Dividiamo il dataset in train e test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modello utilizzato per la calssificazione.\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediacimo le y per i dati di test.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Valutazione della precisione del modello.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = np.array(cm)\n",
    "col_sums = cm.sum(axis=0)\n",
    "cm = cm / col_sums[np.newaxis, :]\n",
    "sns.heatmap(cm, annot=True, cmap='viridis', xticklabels=tools, yticklabels=tools, fmt=\".0%\")\n",
    "plt.title(f\"accuracy:{accuracy}\\n\")\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](classification_matrix.png \"Matrice di Confusione\")\n",
    "\n",
    "Con queste features riusciamo ad ottenere dalle immagini segmentate tramite ground truth un'accuratezza maggiore dell'**80%**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6) Classificazione di oggetti"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo delle immagini che contengono diversi oggetti per provare se il nostro modello funziona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dell'immagine segmentata\n",
    "cwd = os.getcwd()\n",
    "folder_path = os.path.join(cwd, f\"images\\Test_set\")\n",
    "image_files = glob.glob(folder_path + '*.png')\n",
    "\n",
    "with open(f\"{folder_path}\\\\test_gt.json\", 'r') as f:\n",
    "    ground_truth_file = json.load(f)\n",
    "\n",
    "# Estrazione del groun truth.\n",
    "for el in ground_truth_file:\n",
    "    tool_part = ground_truth_file[el]['file_attributes']['tool'].split(',')\n",
    "    if 'gap' in ground_truth_file[el]['file_attributes']:\n",
    "        gap_part = ground_truth_file[el]['file_attributes']['gap'].split(',')\n",
    "\n",
    "    image_path = f\"{folder_path}\\\\{el.split('.')[0]}.png\" \n",
    "    image = cv2.imread(image_path)\n",
    "    truth_mask = np.zeros((520, 520))\n",
    "    for i in range(len(ground_truth_file[el]['regions'])):\n",
    "        x_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_x']\n",
    "        y_pos = ground_truth_file[el]['regions'][i]['shape_attributes']['all_points_y']\n",
    "        vertices = np.array([[x_pos[u], y_pos[u]] for u in range(len(x_pos))])\n",
    "\n",
    "        if str(i+1) in tool_part:\n",
    "                cv2.fillPoly(truth_mask, [vertices], 255)\n",
    "        elif str(i+1) in gap_part:\n",
    "                cv2.fillPoly(truth_mask, [vertices], 0)\n",
    "\n",
    "    # Sovrapposizione della maschera all'immagine originale.\n",
    "    truth_mask = truth_mask.astype(bool)\n",
    "    segmented = np.zeros_like(image)\n",
    "    segmented[truth_mask] = image[truth_mask]\n",
    "\n",
    "    # Disegnare rettangoli intorno agli oggetti trovati.\n",
    "    binary_mask = cv2.cvtColor(segmented, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(binary_mask, 0, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    dim = 260\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "\n",
    "        # Reshape dell'iimagine per l'estrazione delle featus.\n",
    "        cut_img = binary_mask[y:y+h, x:x+w]\n",
    "        while cut_img.shape[0] < dim:\n",
    "            cut_img = np.insert(cut_img, 0, np.array(cut_img.shape[1] * [0]), axis=0)\n",
    "        while cut_img.shape[1] < dim:   \n",
    "            cut_img = np.append(cut_img, np.array(cut_img.shape[0] * [0]).reshape((cut_img.shape[0],1)), axis=1)\n",
    "        cut_img = (cut_img).astype(np.uint8)\n",
    "\n",
    "        # Estrazione delle features per ogni parte segmentata dell'immagine.\n",
    "        euler = euler_number(cut_img)\n",
    "        contours, _ = cv2.findContours(cut_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        moments = cv2.moments(cut_img)\n",
    "        hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        ellipse = cv2.fitEllipse(largest_contour)\n",
    "        major_axis, minor_axis = ellipse[1]\n",
    "        axis_ratio = major_axis / minor_axis\n",
    "\n",
    "        # Classificazione con il modello precedentemente allenato.\n",
    "        input = [euler] + list(hu_moments) + [axis_ratio]\n",
    "        pred_label = clf.predict([input])\n",
    "\n",
    "        # Assegnazione delle etichette predette.\n",
    "        cv2.rectangle(image, (x,y), (x+w,y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(image, pred_label[0], (x, y+h+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Result', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
